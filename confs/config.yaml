defaults:
  - training_scheme: PPO

device: "cpu"

optimization:
  lr: 5e-5
  batchsize: 64
  gae: 0.9

env:
  num_steps: 50
  decay: 0.99