defaults:
  - training_scheme: PPO

device: "cpu"

optimization:
  lr: 3e-4
  batchsize: 64
  gae: 0.95

env:
  num_steps: 100
  decay: 0.99