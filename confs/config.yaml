defaults:
  - training_scheme: PPO

device: "cpu"

optimization:
  lr: 5e-5
  batchsize: 128
  gae: 0.9

model:
  features: 30
  hidden_dim : 512

env:
  num_steps: 50
  decay: 0.99
  harden_gaps: 0.0